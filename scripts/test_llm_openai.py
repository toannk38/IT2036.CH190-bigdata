#!/usr/bin/env python
"""
Test script for LLM Engine with OpenAI API.

This script tests the LLM Engine with real OpenAI API calls.
Make sure to set OPENAI_API_KEY in your .env file before running.

Usage:
    python scripts/test_llm_openai.py
"""

import sys
import os
from datetime import datetime
from unittest.mock import Mock

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.engines.llm_engine import LLMEngine, LLMClient
from src.config import config
from pymongo import MongoClient


def test_llm_client_sentiment():
    """Test LLM client sentiment analysis."""
    print("\n" + "="*80)
    print("Testing LLM Client Sentiment Analysis")
    print("="*80)
    
    # Check if API key is set
    if not config.OPENAI_API_KEY or config.OPENAI_API_KEY == 'your_openai_api_key_here':
        print("\n‚ùå ERROR: OPENAI_API_KEY not set in .env file")
        print("Please set your OpenAI API key in the .env file and try again.")
        return False
    
    print(f"\n‚úì OpenAI API Key found (length: {len(config.OPENAI_API_KEY)})")
    print(f"‚úì Using model: {config.OPENAI_MODEL}")
    
    # Create LLM client with OpenAI
    print("\nüì° Initializing LLM Client with OpenAI API...")
    client = LLMClient(api_key=config.OPENAI_API_KEY, model=config.OPENAI_MODEL, use_openai=True)
    
    # Test cases
    test_cases = [
        {
            "name": "Positive Vietnamese News",
            "text": "Vinamilk (VNM) c√¥ng b·ªë k·∫øt qu·∫£ kinh doanh qu√Ω 4 tƒÉng tr∆∞·ªüng m·∫°nh v·ªõi l·ª£i nhu·∫≠n tƒÉng 25% so v·ªõi c√πng k·ª≥ nƒÉm tr∆∞·ªõc. C√¥ng ty d·ª± ki·∫øn s·∫Ω ti·∫øp t·ª•c m·ªü r·ªông th·ªã tr∆∞·ªùng xu·∫•t kh·∫©u.",
            "expected_sentiment": "positive"
        },
        {
            "name": "Negative Vietnamese News",
            "text": "Vietcombank (VCB) b√°o c√°o l·ª£i nhu·∫≠n gi·∫£m 15% trong qu√Ω 3 do ·∫£nh h∆∞·ªüng c·ªßa n·ª£ x·∫•u tƒÉng cao. Ng√¢n h√†ng ƒëang ƒë·ªëi m·∫∑t v·ªõi nhi·ªÅu kh√≥ khƒÉn trong vi·ªác thu h·ªìi n·ª£.",
            "expected_sentiment": "negative"
        },
        {
            "name": "Neutral Vietnamese News",
            "text": "Vingroup (VIC) t·ªï ch·ª©c ƒë·∫°i h·ªôi c·ªï ƒë√¥ng th∆∞·ªùng ni√™n v√†o ng√†y 15 th√°ng 4. C√°c c·ªï ƒë√¥ng s·∫Ω b·ªè phi·∫øu v·ªÅ c√°c v·∫•n ƒë·ªÅ quan tr·ªçng c·ªßa c√¥ng ty.",
            "expected_sentiment": "neutral"
        },
        {
            "name": "Positive English News",
            "text": "The company reported strong quarterly earnings with revenue growth of 30% and increased market share. Analysts are optimistic about future prospects.",
            "expected_sentiment": "positive"
        },
        {
            "name": "Negative English News",
            "text": "The stock price declined sharply following news of significant losses and declining sales. Investors are concerned about the company's financial health.",
            "expected_sentiment": "negative"
        }
    ]
    
    print("\nüß™ Running sentiment analysis tests...\n")
    
    success_count = 0
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nTest {i}: {test_case['name']}")
        print(f"Text: {test_case['text'][:100]}...")
        print(f"Expected: {test_case['expected_sentiment']}")
        
        try:
            result = client.analyze_sentiment(test_case['text'])
            
            print(f"Result:")
            print(f"  - Sentiment: {result['sentiment']}")
            print(f"  - Score: {result['score']:.2f}")
            print(f"  - Confidence: {result['confidence']:.2f}")
            
            # Check if sentiment matches expected
            if result['sentiment'] == test_case['expected_sentiment']:
                print(f"  ‚úì PASS")
                success_count += 1
            else:
                print(f"  ‚ö† MISMATCH (expected {test_case['expected_sentiment']}, got {result['sentiment']})")
            
        except Exception as e:
            print(f"  ‚ùå ERROR: {e}")
    
    print(f"\n{'='*80}")
    print(f"Results: {success_count}/{len(test_cases)} tests passed")
    print(f"{'='*80}")
    
    return success_count == len(test_cases)


def test_llm_client_summary():
    """Test LLM client summary generation."""
    print("\n" + "="*80)
    print("Testing LLM Client Summary Generation")
    print("="*80)
    
    if not config.OPENAI_API_KEY or config.OPENAI_API_KEY == 'your_openai_api_key_here':
        print("\n‚ùå ERROR: OPENAI_API_KEY not set in .env file")
        return False
    
    print(f"\n‚úì Using model: {config.OPENAI_MODEL}")
    
    # Create LLM client with OpenAI
    print("\nüì° Initializing LLM Client with OpenAI API...")
    client = LLMClient(api_key=config.OPENAI_API_KEY, model=config.OPENAI_MODEL, use_openai=True)
    
    # Test articles
    articles = [
        "Vinamilk c√¥ng b·ªë k·∫øt qu·∫£ kinh doanh qu√Ω 4 tƒÉng tr∆∞·ªüng m·∫°nh v·ªõi l·ª£i nhu·∫≠n tƒÉng 25%.",
        "Vietcombank m·ªü r·ªông m·∫°ng l∆∞·ªõi chi nh√°nh t·∫°i c√°c t·ªânh mi·ªÅn Trung.",
        "Vingroup ra m·∫Øt d·ª± √°n b·∫•t ƒë·ªông s·∫£n m·ªõi t·∫°i H√† N·ªôi v·ªõi quy m√¥ 50 hecta.",
        "FPT Software k√Ω h·ª£p ƒë·ªìng l·ªõn v·ªõi ƒë·ªëi t√°c Nh·∫≠t B·∫£n tr·ªã gi√° 100 tri·ªáu USD.",
        "H√≤a Ph√°t tƒÉng c√¥ng su·∫•t s·∫£n xu·∫•t th√©p ƒë·ªÉ ƒë√°p ·ª©ng nhu c·∫ßu th·ªã tr∆∞·ªùng."
    ]
    
    print(f"\nüß™ Generating summary for {len(articles)} articles...\n")
    
    try:
        summary = client.generate_summary(articles)
        
        print(f"Summary:")
        print(f"{summary}")
        print(f"\n‚úì Summary generated successfully")
        print(f"  Length: {len(summary)} characters")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        return False


def test_llm_engine_full():
    """Test full LLM Engine with mock MongoDB."""
    print("\n" + "="*80)
    print("Testing Full LLM Engine")
    print("="*80)
    
    if not config.OPENAI_API_KEY or config.OPENAI_API_KEY == 'your_openai_api_key_here':
        print("\n‚ùå ERROR: OPENAI_API_KEY not set in .env file")
        return False
    
    print(f"\n‚úì Using model: {config.OPENAI_MODEL}")
    
    # Create mock MongoDB client
    print("\nüîß Creating mock MongoDB client...")
    mock_client = Mock(spec=MongoClient)
    mock_db = Mock()
    mock_news_collection = Mock()
    mock_analysis_collection = Mock()
    
    mock_db.__getitem__ = Mock(side_effect=lambda x: {
        'news': mock_news_collection,
        'llm_analysis': mock_analysis_collection
    }.get(x))
    
    mock_client.__getitem__ = Mock(return_value=mock_db)
    
    # Create sample news articles
    sample_articles = [
        {
            'symbol': 'VNM',
            'title': 'Vinamilk tƒÉng tr∆∞·ªüng m·∫°nh',
            'content': 'Vinamilk c√¥ng b·ªë k·∫øt qu·∫£ kinh doanh qu√Ω 4 tƒÉng tr∆∞·ªüng m·∫°nh v·ªõi l·ª£i nhu·∫≠n tƒÉng 25% so v·ªõi c√πng k·ª≥ nƒÉm tr∆∞·ªõc.',
            'source': 'cafef.vn',
            'published_at': datetime.utcnow().isoformat(),
            'collected_at': datetime.utcnow().isoformat()
        },
        {
            'symbol': 'VNM',
            'title': 'VNM m·ªü r·ªông th·ªã tr∆∞·ªùng',
            'content': 'Vinamilk d·ª± ki·∫øn s·∫Ω ti·∫øp t·ª•c m·ªü r·ªông th·ªã tr∆∞·ªùng xu·∫•t kh·∫©u sang c√°c n∆∞·ªõc ƒê√¥ng Nam √Å.',
            'source': 'vnexpress.net',
            'published_at': datetime.utcnow().isoformat(),
            'collected_at': datetime.utcnow().isoformat()
        },
        {
            'symbol': 'VNM',
            'title': 'C·ªï phi·∫øu VNM tƒÉng gi√°',
            'content': 'C·ªï phi·∫øu Vinamilk tƒÉng 5% trong phi√™n giao d·ªãch h√¥m nay sau th√¥ng tin t√≠ch c·ª±c v·ªÅ k·∫øt qu·∫£ kinh doanh.',
            'source': 'vietstock.vn',
            'published_at': datetime.utcnow().isoformat(),
            'collected_at': datetime.utcnow().isoformat()
        }
    ]
    
    # Mock MongoDB query
    mock_news_collection.find.return_value = sample_articles
    mock_analysis_collection.insert_one.return_value = Mock(inserted_id='mock_id')
    
    # Create LLM Engine with OpenAI
    print("\nüì° Initializing LLM Engine with OpenAI API...")
    engine = LLMEngine(
        mock_client,
        database_name='test_db',
        api_key=config.OPENAI_API_KEY,
        model=config.OPENAI_MODEL,
        use_openai=True
    )
    
    # Analyze news
    print(f"\nüß™ Analyzing news for VNM with {len(sample_articles)} articles...\n")
    
    try:
        result = engine.analyze_news('VNM', lookback_days=7)
        
        if result is None:
            print("‚ùå ERROR: Analysis returned None")
            return False
        
        print(f"Analysis Result:")
        print(f"  Symbol: {result.symbol}")
        print(f"  Timestamp: {result.timestamp}")
        print(f"  Sentiment: {result.sentiment.sentiment}")
        print(f"  Score: {result.sentiment.score:.2f}")
        print(f"  Confidence: {result.sentiment.confidence:.2f}")
        print(f"  Influence Score: {result.influence_score:.2f}")
        print(f"  Articles Analyzed: {result.articles_analyzed}")
        print(f"\n  Summary:")
        print(f"  {result.summary}")
        
        print(f"\n‚úì Analysis completed successfully")
        
        # Verify the result was stored
        if mock_analysis_collection.insert_one.called:
            print(f"‚úì Result stored in MongoDB")
        else:
            print(f"‚ö† Result was not stored in MongoDB")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_fallback_mode():
    """Test LLM Engine in fallback mode (without OpenAI)."""
    print("\n" + "="*80)
    print("Testing LLM Engine in Fallback Mode (Keyword-based)")
    print("="*80)
    
    print("\nüîß Creating LLM Client without OpenAI API...")
    client = LLMClient(api_key=None, use_openai=False)
    
    test_text = "The company reported strong growth and increased profits with positive outlook."
    
    print(f"\nüß™ Testing sentiment analysis (fallback mode)...")
    print(f"Text: {test_text}")
    
    try:
        result = client.analyze_sentiment(test_text)
        
        print(f"\nResult:")
        print(f"  - Sentiment: {result['sentiment']}")
        print(f"  - Score: {result['score']:.2f}")
        print(f"  - Confidence: {result['confidence']:.2f}")
        
        print(f"\n‚úì Fallback mode working correctly")
        return True
        
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        return False


def main():
    """Run all tests."""
    print("\n" + "="*80)
    print("LLM Engine OpenAI Integration Test Suite")
    print("="*80)
    
    # Check environment
    print("\nüìã Environment Check:")
    print(f"  OPENAI_API_KEY: {'‚úì Set' if config.OPENAI_API_KEY and config.OPENAI_API_KEY != 'your_openai_api_key_here' else '‚ùå Not set'}")
    print(f"  OPENAI_MODEL: {config.OPENAI_MODEL}")
    
    results = []
    
    # Test 1: Fallback mode (always works)
    results.append(("Fallback Mode", test_fallback_mode()))
    
    # Test 2-4: OpenAI tests (only if API key is set)
    if config.OPENAI_API_KEY and config.OPENAI_API_KEY != 'your_openai_api_key_here':
        results.append(("Sentiment Analysis", test_llm_client_sentiment()))
        results.append(("Summary Generation", test_llm_client_summary()))
        results.append(("Full Engine Test", test_llm_engine_full()))
    else:
        print("\n‚ö† Skipping OpenAI tests (API key not set)")
        print("To test with OpenAI, set OPENAI_API_KEY in your .env file")
    
    # Print summary
    print("\n" + "="*80)
    print("Test Summary")
    print("="*80)
    
    for test_name, passed in results:
        status = "‚úì PASS" if passed else "‚ùå FAIL"
        print(f"{test_name}: {status}")
    
    total_passed = sum(1 for _, passed in results if passed)
    print(f"\nTotal: {total_passed}/{len(results)} tests passed")
    print("="*80)
    
    return total_passed == len(results)


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
