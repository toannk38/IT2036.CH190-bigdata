version: '3.8'

# Vietnam Stock AI Backend - Complete Docker Compose Configuration
# This file orchestrates all services: Infrastructure + Application
# Requirements: 10.1, 10.2, 10.3, 10.4

services:
  # Airflow Standalone - Workflow orchestration with SQLite backend
  airflow:
    build:
      context: .
      dockerfile: docker/Dockerfile.airflow
    container_name: stock-ai-airflow
    hostname: airflow
    ports:
      - "8080:8080"
    environment:
      # Airflow Core Configuration
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "false"
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30
      # Webserver Configuration
      AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT: 300
      AIRFLOW__WEBSERVER__WORKER_REFRESH_BATCH_SIZE: 0
      AIRFLOW__WEBSERVER__WORKER_REFRESH_INTERVAL: 0
      AIRFLOW__WEBSERVER__RELOAD_ON_PLUGIN_CHANGE: "false"
      # Application Configuration
      MONGODB_URI: mongodb://mongodb:27017
      MONGODB_DATABASE: vietnam_stock_ai
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_PRICE_TOPIC: stock_prices_raw
      KAFKA_NEWS_TOPIC: stock_news_raw
      PYTHONPATH: /opt/airflow
      LOG_LEVEL: INFO
    volumes:
      - ./dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - ./data:/opt/airflow/data
      - airflow_logs:/opt/airflow/logs
      - airflow_data:/opt/airflow
    networks:
      - stock-ai-network
    command: >
      bash -c "
      echo 'Cleaning up any stale PID files...' &&
      rm -f /opt/airflow/airflow-webserver.pid /opt/airflow/airflow-scheduler.pid &&
      echo 'Initializing Airflow database...' &&
      airflow db init &&
      echo 'Creating admin user...' &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com 2>/dev/null || true &&
      echo 'Starting Airflow standalone...' &&
      airflow standalone
      "
    restart: unless-stopped

  # Kafka Consumer - Consumes messages from Kafka and stores in MongoDB
  kafka-consumer:
    build:
      context: .
      dockerfile: docker/Dockerfile.consumer
    container_name: stock-ai-consumer
    hostname: kafka-consumer
    environment:
      MONGODB_URI: mongodb://mongodb:27017
      MONGODB_DATABASE: vietnam_stock_ai
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_PRICE_TOPIC: stock_prices_raw
      KAFKA_NEWS_TOPIC: stock_news_raw
      KAFKA_GROUP_ID: stock-ai-consumer-group
      LOG_LEVEL: INFO
      PYTHONPATH: /app
    volumes:
      - ./src:/app/src
      - ./data:/app/data
    networks:
      - stock-ai-network
    restart: unless-stopped

  # API Service - REST API for client applications
  api-service:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    container_name: stock-ai-api
    hostname: api-service
    ports:
      - "8000:8000"
    environment:
      MONGODB_URI: mongodb://mongodb:27017
      MONGODB_DATABASE: vietnam_stock_ai
      LOG_LEVEL: INFO
      PYTHONPATH: /app
      API_HOST: 0.0.0.0
      API_PORT: 8000
    volumes:
      - ./src:/app/src
      - ./data:/app/data
    networks:
      - stock-ai-network
    restart: unless-stopped

# ============================================================================
# NETWORKS
# ============================================================================
networks:
  stock-ai-network:
    name: stock-ai-network
    driver: bridge

# ============================================================================
# VOLUMES
# ============================================================================
volumes:
  kafka_data:
    driver: local
  mongodb_data:
    driver: local
  mongodb_config:
    driver: local
  airflow_logs:
    driver: local
  airflow_data:
    driver: local
 